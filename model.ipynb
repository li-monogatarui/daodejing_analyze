{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lau = pd.read_csv(\"Cleaning/lau.csv\")\n",
    "legge = pd.read_csv(\"Cleaning/legge.csv\")\n",
    "\n",
    "lau['txt'] = lau['txt'].map(lambda x: re.sub(r'[^\\w\\s]', ' ', x))\n",
    "lau['txt'] = lau['txt'].apply(lambda x: x.lower())\n",
    "\n",
    "legge['txt'] = legge['txt'].map(lambda x: re.sub(r'[^\\w\\s]', ' ', x))\n",
    "legge['txt'] = legge['txt'].apply(lambda x: x.lower())\n",
    "\n",
    "lau['seq_words'] = lau['txt'].apply(lambda x: x.split())\n",
    "legge['seq_words'] = legge['txt'].apply(lambda x: x.split())\n",
    "\n",
    "set1 = lau[\"seq_words\"]\n",
    "set2 = legge[\"seq_words\"]\n",
    "\n",
    "train_label = []\n",
    "train_tokens = []\n",
    "test_label = []\n",
    "test_tokens = []\n",
    "\n",
    "for i in range(0, 70):\n",
    "    train_tokens.append(set1[i])\n",
    "    train_label.append(0)\n",
    "    train_tokens.append(set2[i])\n",
    "    train_label.append(1)\n",
    "\n",
    "\n",
    "for i in range(70, 81):\n",
    "    test_tokens.append(set1[i])\n",
    "    test_label.append(0)\n",
    "    test_tokens.append(set2[i])\n",
    "    test_label.append(1)\n",
    "\n",
    "train_data = pd.DataFrame({'label': train_label, 'seq_words': train_tokens})\n",
    "test_data = pd.DataFrame({'label': test_label, 'seq_words': test_tokens})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>seq_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[the, way, that, can, be, spoken, of, is, not,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[the, tao, that, can, be, trodden, is, not, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[the, whole, world, recognizes, the, beautiful...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>[all, in, the, world, know, the, beauty, of, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[not, to, honor, men, of, worth, will, keep, t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                          seq_words\n",
       "0      0  [the, way, that, can, be, spoken, of, is, not,...\n",
       "1      1  [the, tao, that, can, be, trodden, is, not, th...\n",
       "2      0  [the, whole, world, recognizes, the, beautiful...\n",
       "3      1  [all, in, the, world, know, the, beauty, of, t...\n",
       "4      0  [not, to, honor, men, of, worth, will, keep, t..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('be', 198), ('he', 192), ('one', 147), ('who', 146), ('are', 141), ('no', 135), ('his', 132), ('will', 122), ('as', 119), ('its', 116), ('them', 114), ('all', 113), ('when', 112), ('i', 94), ('by', 93), ('from', 92), ('their', 91), ('way', 90), ('there', 88), ('with', 86)]\n"
     ]
    }
   ],
   "source": [
    "words = train_data['seq_words'].tolist()\n",
    "tokens_list = []\n",
    "for i in words:\n",
    "    tokens_list.extend(i)\n",
    "    \n",
    "count_tokens = Counter(tokens_list)\n",
    "sorted_tokens = count_tokens.most_common(len(tokens_list))\n",
    "print(sorted_tokens[10:30])\n",
    "tokens_top = sorted_tokens[:1000]\n",
    "    \n",
    "tokens2index = {w:i+2 for i, (w,c) in enumerate(tokens_top)}\n",
    "\n",
    "tokens2index['<pad>'] = 0\n",
    "tokens2index['<unk>'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_word2index(x,tokens2index):\n",
    "    input_tokens = [tokens2index.get(w,1) for w in x]\n",
    "\n",
    "    return input_tokens\n",
    "\n",
    "train_data['input_x'] = train_data['seq_words'].apply(lambda x: encode_word2index(x, tokens2index))\n",
    "\n",
    "test_data['input_x'] = test_data['seq_words'].apply(lambda x: encode_word2index(x, tokens2index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>seq_words</th>\n",
       "      <th>input_x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[the, way, that, can, be, spoken, of, is, not,...</td>\n",
       "      <td>[2, 29, 11, 46, 12, 770, 6, 3, 9, 2, 143, 29, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[the, tao, that, can, be, trodden, is, not, th...</td>\n",
       "      <td>[2, 35, 11, 46, 12, 1, 3, 9, 2, 282, 5, 190, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[the, whole, world, recognizes, the, beautiful...</td>\n",
       "      <td>[2, 232, 75, 780, 2, 284, 20, 2, 284, 42, 34, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>[all, in, the, world, know, the, beauty, of, t...</td>\n",
       "      <td>[23, 8, 2, 75, 61, 2, 784, 6, 2, 284, 5, 8, 11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[not, to, honor, men, of, worth, will, keep, t...</td>\n",
       "      <td>[9, 4, 792, 56, 6, 460, 19, 85, 2, 45, 27, 793...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>1</td>\n",
       "      <td>[he, who, in, tao, s, wars, has, skill, assume...</td>\n",
       "      <td>[13, 15, 8, 35, 65, 1, 43, 212, 1, 17, 1, 1, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0</td>\n",
       "      <td>[the, strategists, have, a, saying, i, dare, n...</td>\n",
       "      <td>[2, 1, 37, 10, 268, 25, 236, 9, 1, 2, 423, 38,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>1</td>\n",
       "      <td>[a, master, of, the, art, of, war, has, said, ...</td>\n",
       "      <td>[10, 370, 6, 2, 744, 6, 227, 43, 204, 25, 47, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0</td>\n",
       "      <td>[my, words, are, very, easy, to, understand, a...</td>\n",
       "      <td>[171, 145, 16, 250, 114, 4, 584, 5, 250, 114, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>1</td>\n",
       "      <td>[my, words, are, very, easy, to, know, and, ve...</td>\n",
       "      <td>[171, 145, 16, 250, 114, 4, 61, 5, 250, 114, 4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>140 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                          seq_words  \\\n",
       "0        0  [the, way, that, can, be, spoken, of, is, not,...   \n",
       "1        1  [the, tao, that, can, be, trodden, is, not, th...   \n",
       "2        0  [the, whole, world, recognizes, the, beautiful...   \n",
       "3        1  [all, in, the, world, know, the, beauty, of, t...   \n",
       "4        0  [not, to, honor, men, of, worth, will, keep, t...   \n",
       "..     ...                                                ...   \n",
       "135      1  [he, who, in, tao, s, wars, has, skill, assume...   \n",
       "136      0  [the, strategists, have, a, saying, i, dare, n...   \n",
       "137      1  [a, master, of, the, art, of, war, has, said, ...   \n",
       "138      0  [my, words, are, very, easy, to, understand, a...   \n",
       "139      1  [my, words, are, very, easy, to, know, and, ve...   \n",
       "\n",
       "                                               input_x  \n",
       "0    [2, 29, 11, 46, 12, 770, 6, 3, 9, 2, 143, 29, ...  \n",
       "1    [2, 35, 11, 46, 12, 1, 3, 9, 2, 282, 5, 190, 3...  \n",
       "2    [2, 232, 75, 780, 2, 284, 20, 2, 284, 42, 34, ...  \n",
       "3    [23, 8, 2, 75, 61, 2, 784, 6, 2, 284, 5, 8, 11...  \n",
       "4    [9, 4, 792, 56, 6, 460, 19, 85, 2, 45, 27, 793...  \n",
       "..                                                 ...  \n",
       "135  [13, 15, 8, 35, 65, 1, 43, 212, 1, 17, 1, 1, 1...  \n",
       "136  [2, 1, 37, 10, 268, 25, 236, 9, 1, 2, 423, 38,...  \n",
       "137  [10, 370, 6, 2, 744, 6, 227, 43, 204, 25, 47, ...  \n",
       "138  [171, 145, 16, 250, 114, 4, 584, 5, 250, 114, ...  \n",
       "139  [171, 145, 16, 250, 114, 4, 61, 5, 250, 114, 4...  \n",
       "\n",
       "[140 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 100\n",
    "def _pad_truncate_seq(x,seq_len):\n",
    "    if len(x) >= seq_len:\n",
    "        return x[:seq_len]\n",
    "    else:\n",
    "        return x+[0]*(seq_len-len(x))\n",
    "\n",
    "train_data['input_x'] = train_data['input_x'].apply(lambda x: _pad_truncate_seq(x, seq_len))\n",
    "\n",
    "test_data['input_x'] = test_data['input_x'].apply(lambda x: _pad_truncate_seq(x, seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>seq_words</th>\n",
       "      <th>input_x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[the, way, that, can, be, spoken, of, is, not,...</td>\n",
       "      <td>[2, 29, 11, 46, 12, 770, 6, 3, 9, 2, 143, 29, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[the, tao, that, can, be, trodden, is, not, th...</td>\n",
       "      <td>[2, 35, 11, 46, 12, 1, 3, 9, 2, 282, 5, 190, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[the, whole, world, recognizes, the, beautiful...</td>\n",
       "      <td>[2, 232, 75, 780, 2, 284, 20, 2, 284, 42, 34, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>[all, in, the, world, know, the, beauty, of, t...</td>\n",
       "      <td>[23, 8, 2, 75, 61, 2, 784, 6, 2, 284, 5, 8, 11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[not, to, honor, men, of, worth, will, keep, t...</td>\n",
       "      <td>[9, 4, 792, 56, 6, 460, 19, 85, 2, 45, 27, 793...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                          seq_words  \\\n",
       "0      0  [the, way, that, can, be, spoken, of, is, not,...   \n",
       "1      1  [the, tao, that, can, be, trodden, is, not, th...   \n",
       "2      0  [the, whole, world, recognizes, the, beautiful...   \n",
       "3      1  [all, in, the, world, know, the, beauty, of, t...   \n",
       "4      0  [not, to, honor, men, of, worth, will, keep, t...   \n",
       "\n",
       "                                             input_x  \n",
       "0  [2, 29, 11, 46, 12, 770, 6, 3, 9, 2, 143, 29, ...  \n",
       "1  [2, 35, 11, 46, 12, 1, 3, 9, 2, 282, 5, 190, 3...  \n",
       "2  [2, 232, 75, 780, 2, 284, 20, 2, 284, 42, 34, ...  \n",
       "3  [23, 8, 2, 75, 61, 2, 784, 6, 2, 284, 5, 8, 11...  \n",
       "4  [9, 4, 792, 56, 6, 460, 19, 85, 2, 45, 27, 793...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv('training_data.csv', index = False)\n",
    "test_data.to_csv('test_data.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from ast import literal_eval\n",
    "import torch.autograd as autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadData(Dataset):\n",
    "    def __init__(self, filename):\n",
    "        self.df = pd.read_csv(filename, converters={'input_x': literal_eval})\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        ## load the input features and labels\n",
    "        input_x = self.df.loc[index, 'input_x']\n",
    "        label = self.df.loc[index, 'label']\n",
    "\n",
    "        return torch.tensor(input_x), torch.tensor(label,dtype=torch.float)\n",
    "    \n",
    "\n",
    "training_set = LoadData('training_data.csv')\n",
    "test_set = LoadData('test_data.csv')\n",
    "training_generator = torch.utils.data.DataLoader(training_set, batch_size = 70, shuffle=True, num_workers = 1)\n",
    "test_generator = torch.utils.data.DataLoader(test_set, batch_size = 70, shuffle=True, num_workers = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.LoadData at 0x7fe51bba0a90>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokens2index)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    # LSTM Algorithm built with help from Professor Huajie Shao's lecture slides\n",
    "\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim,\\\n",
    "        hidden_dim, n_layers, input_len):\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_size = output_size  # y_out size = 1\n",
    "        self.n_layers = n_layers   # layers of LSTM\n",
    "        self.hidden_dim = hidden_dim  # hidden dim of LSTM\n",
    "        self.input_len = input_len # input features\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.init_weights()\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.pool = nn.MaxPool1d(self.input_len)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
    "    \n",
    "    ## initial hidden state and cell state\n",
    "    def _init_hidden(self, batch_size):\n",
    "        return(autograd.Variable(torch.randn(self.n_layers, batch_size, self.hidden_dim)).to(device),\n",
    "                autograd.Variable(torch.randn(self.n_layers, batch_size, self.hidden_dim)).to(device)\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        hidden_cell = self._init_hidden(batch_size)\n",
    "\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embeds, hidden_cell)\n",
    "        lstm_out = lstm_out.permute(0,2,1)\n",
    "        out = self.pool(lstm_out)\n",
    "        out = out.view(out.size(0),-1)\n",
    "        \n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = self.fc(out)\n",
    "        out = self.sigmoid(out)\n",
    "        out = out[:,0]\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = 2\n",
    "hidden_dim = 50\n",
    "output_size = 1\n",
    "learning_rate = 0.003\n",
    "input_len = 100\n",
    "embedding_dim = 100\n",
    "mode = 'train'\n",
    "num_epoches = 40\n",
    "clip = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMModel(\n",
       "  (embedding): Embedding(1002, 100)\n",
       "  (lstm): LSTM(100, 50, num_layers=2, batch_first=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (pool): MaxPool1d(kernel_size=100, stride=100, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc): Linear(in_features=50, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LSTMModel(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, input_len)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fun = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_accuracy(y_pred, y_batch, batch_size):\n",
    "    accy = ((y_pred==y_batch).sum().item())/batch_size\n",
    "    return accy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbingrui-ben-li\u001b[0m (\u001b[33mbenbingruili\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/Assignment4/wandb/run-20240520_131630-x5lq8kog</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/benbingruili/DaoDeJing/runs/x5lq8kog' target=\"_blank\">LSTM</a></strong> to <a href='https://wandb.ai/benbingruili/DaoDeJing' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/benbingruili/DaoDeJing' target=\"_blank\">https://wandb.ai/benbingruili/DaoDeJing</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/benbingruili/DaoDeJing/runs/x5lq8kog' target=\"_blank\">https://wandb.ai/benbingruili/DaoDeJing/runs/x5lq8kog</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 2 loss: 0.697630763053894\n",
      "step: 4 loss: 0.6973266005516052\n",
      "step: 6 loss: 0.6953089833259583\n",
      "step: 8 loss: 0.7024322748184204\n",
      "step: 10 loss: 0.6936585307121277\n",
      "step: 12 loss: 0.6784517765045166\n",
      "step: 14 loss: 0.6844329833984375\n",
      "step: 16 loss: 0.6999346017837524\n",
      "step: 18 loss: 0.674450159072876\n",
      "step: 20 loss: 0.6917302012443542\n",
      "step: 22 loss: 0.6929349899291992\n",
      "step: 24 loss: 0.6831426024436951\n",
      "step: 26 loss: 0.6816210150718689\n",
      "step: 28 loss: 0.6780406832695007\n",
      "step: 30 loss: 0.6762481927871704\n",
      "step: 32 loss: 0.6636157035827637\n",
      "step: 34 loss: 0.6251184940338135\n",
      "step: 36 loss: 0.5898539423942566\n",
      "step: 38 loss: 0.5021918416023254\n",
      "step: 40 loss: 0.3811611831188202\n",
      "step: 42 loss: 0.29215574264526367\n",
      "step: 44 loss: 0.28130656480789185\n",
      "step: 46 loss: 0.19154000282287598\n",
      "step: 48 loss: 0.16102786362171173\n",
      "step: 50 loss: 0.1250331550836563\n",
      "step: 52 loss: 0.13284893333911896\n",
      "step: 54 loss: 0.08262196183204651\n",
      "step: 56 loss: 0.11330393701791763\n",
      "step: 58 loss: 0.13333888351917267\n",
      "step: 60 loss: 0.07734315097332001\n",
      "step: 62 loss: 0.07509750127792358\n",
      "step: 64 loss: 0.05620096996426582\n",
      "step: 66 loss: 0.034785788506269455\n",
      "step: 68 loss: 0.0540909543633461\n",
      "step: 70 loss: 0.03145911172032356\n",
      "step: 72 loss: 0.029785210266709328\n",
      "step: 74 loss: 0.021818596869707108\n",
      "step: 76 loss: 0.02215515822172165\n",
      "step: 78 loss: 0.018474888056516647\n",
      "step: 80 loss: 0.014708062633872032\n",
      "testing accy:  0.7727272727272727\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    global_step = 0\n",
    "    print(\"Started\")\n",
    "    wandb.init(project='daodejing', name='LSTM')\n",
    "    if mode == 'train':\n",
    "        model.train()\n",
    "        for epoches in range(num_epoches):\n",
    "            for x_batch, y_labels in training_generator:\n",
    "                global_step += 1\n",
    "                x_batch, y_labels = x_batch.to(device), y_labels.to(device)\n",
    "           \n",
    "                y_out = model(x_batch)\n",
    "\n",
    "                loss = loss_fun(y_out, y_labels)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                ## clip_grad_norm helps prevent the exploding gradient problem in LSTMs.\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "                optimizer.step()\n",
    "                if global_step%2==0:\n",
    "                    print('step: {} loss: {}'.format(global_step, loss.item()))\n",
    "                    wandb.log({'step': global_step, 'loss': loss.item()})\n",
    "            \n",
    "        total = 0\n",
    "        accy_count = 0\n",
    "    \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_labels in test_generator:\n",
    "                x_batch, y_labels = x_batch.to(device), y_labels.to(device)\n",
    "                y_out = model(x_batch)\n",
    "                y_pred = torch.round(y_out)\n",
    "\n",
    "                total += len(y_labels)\n",
    "                accy_count += _compute_accuracy(y_pred, y_labels, 150) * 150\n",
    "\n",
    "        accy = accy_count/total\n",
    "        print(\"testing accy: \", accy)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tmain()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
